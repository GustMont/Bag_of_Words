{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfQCzopLq+/gucaTUs3NaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GustMont/Bag_of_Words/blob/main/ATIVIDADE_Bag_of_Words_Gustavo_Monteiro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ATIVIDADE:Bag of Words**"
      ],
      "metadata": {
        "id": "hdZ9SSSfmLkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aluno: Gustavo Monteiro"
      ],
      "metadata": {
        "id": "h0zASEKkWeqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leia a implementação guiada do tutorial do Bag of Words disponível no Kaggle (https://www.kaggle.com/code/vipulgandhi/bag-of-words-model-for-beginners). Desenvolva seu código Python no Google Colab a partir do código apresentado no tutorial, **organize as etapas com suas respectivas descrições, e realize testes** com 25 frases diferentes em inglês e 25 frases diferentes em português (para este último passo, é permitido utilizar outras bibliotecas)."
      ],
      "metadata": {
        "id": "HqcY0eRpmMr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importação de bibliotecas:"
      ],
      "metadata": {
        "id": "pjLKDzoD1d-R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JosO1Apjl19J"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frases utilizadas para a atividade (Português):"
      ],
      "metadata": {
        "id": "UMD_acQ41iww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "textPt = [\n",
        "    \"Quero ficar rica\",\n",
        "    \"Eu estou rica\",\n",
        "    \"Sou rica em conhecimento\",\n",
        "    \"Ele está rica\",\n",
        "    \"Rica estamos\",\n",
        "    \"Riqueza engrandece\",\n",
        "    \"A rica está hoje?\",\n",
        "    \"Não gosto de gente rica\",\n",
        "    \"Rica em sociedade, pobre em conhecimento\",\n",
        "    \"Quero ele pobre\",\n",
        "    \"Não gosto de conhecimento hoje\",\n",
        "    \"Rica, pobre. Gente, estou hoje\",\n",
        "    \"Transformar pobre em rico!\",\n",
        "    \"Não estamos em sociedade.\",\n",
        "    \"Hoje estou rica em conhecimento.\",\n",
        "    \"Ele está pobre, não em conhecimento.\",\n",
        "    \"Hoje quero conhecimento, não sociedade.\",\n",
        "    \"A sociedade não está rica em conhecimento.\",\n",
        "    \"Conhecimento em sociedade.\",\n",
        "    \"Hoje não gosto de riqueza. Quero transformar em conhecimento\",\n",
        "    \"Riqueza de conhecimento para pobre.\",\n",
        "    \"Eu estou, não sou.\",\n",
        "    \"Estamos, não estamos?\",\n",
        "    \"Sociedade rica.\",\n",
        "    \"Transformar pobre em conhecimento e riqueza em socidade.\"]\n",
        ""
      ],
      "metadata": {
        "id": "vxDuH0fzodBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformar lista em vetor de palavras (tokenizar):"
      ],
      "metadata": {
        "id": "J8h030k01qOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste processo, será instanciado a variável para tokenizar as frases, e construir a lista de vocabulários."
      ],
      "metadata": {
        "id": "qZoFCbsyUwC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizerPt = CountVectorizer()\n",
        "vectorizerPt.fit(textPt)\n",
        "print(sorted(vectorizerPt.vocabulary_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYykgpFS1pVv",
        "outputId": "8679da4c-9148-461e-d61d-0da1fa9fd223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['conhecimento', 'de', 'ele', 'em', 'engrandece', 'estamos', 'estou', 'está', 'eu', 'ficar', 'gente', 'gosto', 'hoje', 'não', 'para', 'pobre', 'quero', 'rica', 'rico', 'riqueza', 'socidade', 'sociedade', 'sou', 'transformar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matriz de ocorrência de palavras em relação às frases utilizadas"
      ],
      "metadata": {
        "id": "XOxSJK0KVDxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essa matriz contém as palavras utilizadas na lista, construída com as frases que foram tokenizadas, que mostram a contagem de palavras.\n"
      ],
      "metadata": {
        "id": "5rNgnLyQVTcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = vectorizerPt.transform(textPt)\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mk7t2ysnq0i",
        "outputId": "f0475a2a-bc15-4f29-bb51-47b9af630cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25, 24)\n",
            "[[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0]\n",
            " [0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0]\n",
            " [1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
            " [1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1]\n",
            " [0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
            " [1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0]\n",
            " [1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0]\n",
            " [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 1]\n",
            " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 2 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
            " [1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frases utilizadas para a atividade (Inglês):"
      ],
      "metadata": {
        "id": "skU_HDOGVz8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "textEn = [\n",
        "    \"I love pizza\",\n",
        "    \"Pizza is life\",\n",
        "    \"Life without pizza is sad\",\n",
        "    \"Sadness can be pizza\",\n",
        "    \"Pizza makes me happy\",\n",
        "    \"Happy people love pizza\",\n",
        "    \"Pizza is amazing\",\n",
        "    \"Pizza exist\",\n",
        "    \"pizza or not pizza?\",\n",
        "    \"Pizza is my food\",\n",
        "    \"Food means pizza\",\n",
        "    \"Pizza is a language\",\n",
        "    \"Language of love? Pizza!\",\n",
        "    \"Pizza!\",\n",
        "    \"Hot pizza at my door\",\n",
        "    \"Pizza time!\",\n",
        "    \"Time flies when pizza\",\n",
        "    \"Pizza anyone?\",\n",
        "    \"Anyone who doesn't like pizza is happy\",\n",
        "    \"Not liking pizza\",\n",
        "    \"Pizza, right?\",\n",
        "    \"Right, I want pizza\",\n",
        "    \"Pizza are real\",\n",
        "    \"Reality is pizza\",\n",
        "    \"Pizza = happiness\"]"
      ],
      "metadata": {
        "id": "3aGrnvR1zwKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformar lista em vetor de palavras (tokenizar):"
      ],
      "metadata": {
        "id": "yDiOMw1hV6lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste processo, será instanciado a variável para tokenizar as frases, e construir a lista de vocabulários."
      ],
      "metadata": {
        "id": "hW9epR5WWQSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizerEn = CountVectorizer()\n",
        "vectorizerEn.fit(textEn)\n",
        "print(sorted(vectorizerEn.vocabulary_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPRpOXafWAXK",
        "outputId": "499d1ea9-b073-4694-a303-6edc572c4d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['amazing', 'anyone', 'are', 'at', 'be', 'can', 'doesn', 'door', 'exist', 'flies', 'food', 'happiness', 'happy', 'hot', 'is', 'language', 'life', 'like', 'liking', 'love', 'makes', 'me', 'means', 'my', 'not', 'of', 'or', 'people', 'pizza', 'real', 'reality', 'right', 'sad', 'sadness', 'time', 'want', 'when', 'who', 'without']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matriz de ocorrência de palavras em relação às frases utilizadas"
      ],
      "metadata": {
        "id": "qt2WLOwfWJ9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essa matriz contém as palavras utilizadas na lista, construída com as frases que foram tokenizadas, que mostram a contagem de palavras."
      ],
      "metadata": {
        "id": "wk84zqWIWPUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = vectorizerEn.transform(textEn)\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "837Ob0w1zzKq",
        "outputId": "3c3f9fef-6ba1-4f4f-af53-6cf8a03d7709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25, 39)\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
            "  0 0 1]\n",
            " [0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 2 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            "  1 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1\n",
            "  0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]]\n"
          ]
        }
      ]
    }
  ]
}